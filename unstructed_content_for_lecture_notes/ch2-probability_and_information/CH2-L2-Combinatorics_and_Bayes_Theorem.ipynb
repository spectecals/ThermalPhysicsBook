{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm, gamma"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Probability Theory for Thermal Physics - Lecture 2",
   "id": "257c73aa5f0f39d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```{admonition} Relevant readings and preparation\n",
    ":class: reading\n",
    "\n",
    "- Concepts in Thermal Physics: Chapter 3: 3.1-3.8: pg. 20-28"
   ],
   "id": "e0e5b4bbf046d669"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```{admonition} Learning outcomes:\n",
    ":class: outcomes\n",
    "- Describe the properties and applications of the **binomial distribution** in modelling discrete physical events.\n",
    "- Apply **Bayes’ theorem** to update parameters of probability distributions in response to new information."
   ],
   "id": "7b65c9991928da6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Binomial Distribution\n",
    "\n",
    "<!-- stef: we've covered combinatorials in chapter 1, i'm going to hash this bit out for now\n",
    "### Counting combinations\n",
    "Before introducing the binomial distribution, it helps to recall some basic **combinatorics**—the mathematics of counting how many ways outcomes can be arranged.\n",
    "\n",
    "Suppose you have *n* objects, and you want to choose *k* of them without caring about order in which they are selected.\n",
    "The number of distinct selections, aka **combinations**, is:\n",
    "\n",
    "$$\n",
    "{n \\choose k} = \\frac{n! }{k! (n - k)! }\n",
    "$$\n",
    "\n",
    "where \"! \" denotes the factorial operation (e. g 5! = 5 × 4 × 3 × 2 × 1).\n",
    "For example, there are ${4 \\choose 2} = 6$ ways to pick 2 numbers from the set {1, 2 ,3 4}: (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4).\n",
    "\n",
    "This quantity appears throughout probability theory because it counts how many different sequences can contain *k* particular outcomes (like *successes*) among *n* trials.\n",
    "\n",
    "--- -->\n",
    "\n",
    "Consider an experiment with only two possible outcomes, **success** and **failure**. This is called a **Bernoulli trial**.\n",
    "\n",
    "- Let the probability of success be *p*.\n",
    "- Then the probability of failure is (1 − *p*).\n",
    "\n",
    "If we assign the value 1 to a success and 0 to a failure, the expectation and variance of a single trial are:\n",
    "\n",
    "$$\n",
    "\\langle x \\rangle = p, \\qquad\n",
    "\\sigma_x^2 = p(1 - p)\n",
    "$$\n",
    "\n",
    "Now imagine performing $n$ independent Bernoulli trials; for example, flipping a coin $n$ times where we count $k$ heads. Let us consider \"heads\" as a success.\n",
    "\n",
    "There are two ingredients to the probability of obtaining *k* successes:\n",
    "\n",
    "1. **The probability of one specific sequence** with *k* successes and *n − k* failures:\n",
    "\n",
    " $$\n",
    " p^k (1 - p)^{n - k}\n",
    " $$\n",
    "\n",
    "2. **The number of possible sequences** with exactly *k* successes:\n",
    "\n",
    " $$\n",
    " {n \\choose k}\n",
    " $$\n",
    "\n",
    "Multiplying these gives the **binomial probability** of observing *k* successes given *n* trials:\n",
    "\n",
    "$$\n",
    "P(n, k) = {n \\choose k} p^k (1 - p)^{n - k}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Properties\n",
    "From the definition, one can show that the probability of all combinations of successes and failures sums to unity:\n",
    "\n",
    "$$\n",
    "\\sum_{k=0}^{n} P(n, k) = 1,\n",
    "$$\n",
    "\n",
    "so the probabilities are properly normalised.\n",
    "The **mean** and **variance** of *k* are:\n",
    "\n",
    "$$\n",
    "\\langle k \\rangle = np, \\qquad\n",
    "\\sigma_k^2 = np(1 - p)\n",
    "$$\n",
    "\n",
    "As *n* increases, both the mean and standard deviation grow, but the **fractional width**\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma_k}{\\langle k \\rangle} =\n",
    "\\sqrt{\\frac{1 - p}{np}}\n",
    "$$\n",
    "\n",
    "decreases, meaning the distribution becomes more sharply peaked around *k = np*.\n",
    "\n",
    "---\n",
    "\n",
    "```{admonition} Example - coin tossing\n",
    ":class: example\n",
    "\n",
    "For a fair coin, $p = \\frac{1}{2}$.\n",
    "\n",
    "- For 16 tosses, expected heads: $\\langle k \\rangle = 8$; $\\sigma = 2$.\n",
    "- For $10^{20}$ tosses, expected heads: $\\langle k \\rangle = 5\\times10^{19}$; $\\sigma = 5\\times10^{9}$ - ten orders of magnitude smaller relative to the mean.\n",
    "\n",
    "Thus, as the number of trials increases, the relative fluctuations around the mean gradually become negligible.\n",
    "\n",
    "```\n",
    "\n",
    "```{admonition} Example: 1D random walk\n",
    ":class: example\n",
    "\n",
    "A one-dimensional **random walk** can be viewed as successive Bernoulli trials:\n",
    "each step is either forward (+L) or backward (−L) with equal probability $p = \\frac{1}{2}$.\n",
    "\n",
    "After *n* steps, if *k* are forward, the net displacement is\n",
    "\n",
    "$$\n",
    "x = (2k - n)L.\n",
    "$$\n",
    "\n",
    "Using the binomial results,\n",
    "\n",
    "$$\n",
    "\\langle x \\rangle = 0, \\qquad\n",
    "\\langle x^2 \\rangle = nL^2, \\qquad\n",
    "\\sigma_x = \\sqrt{n}\\, L,\n",
    "$$\n",
    "\n",
    "showing that the root-mean-square displacement grows as $\\sqrt{n}$ - a key result that later links to Brownian motion.\n",
    "```"
   ],
   "id": "da8d9c7d44c7ea3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Should you gamble at the casino? (Roulette wheel)\n",
    "\n",
    "Generally inadvisable, unless declaring bankruptcy is the objective.\n",
    "\n",
    "[[Roulette wheel figure stock image]]\n",
    "\n",
    "On a roulette wheel, there are 36 segments which can be bet on, along with the zero segment, meaning there are 37 possible outcomes. There are equally many red tiles as there are black, so the probability of landing on either colour is $\\frac{18}{37}$. Note that in the American version of the game, there is a second zero tile labelled '00', making the American version of Roulette a worse proposition financially than the European game.\n",
    "\n",
    "To play, you must pay an entry fee (£1 here) and select a colour. If your colour comes up, your bet is returned to you along with an additional £1. This begs the question, *on average*, how much money do you expect to win from your £1? We can calculate the expected return as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\langle £ \\rangle\n",
    "&= \\sum_{outcomes} \\text{probability of outcome} \\times \\text{return from corresponding outcome} \\\\\n",
    "&= \\left(\\frac{18}{37} \\times £2 \\right) \\times \\left(\\frac{18}{37} \\times £0 \\right) = 0.973 = £0.97\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This means that *on average*, you lose £0.03 for every £1 you bet. Were you to play only once, you have a $\\frac{18}{37} = 48.6\\%$ chance of winning, and there's a $\\frac{19}{37} = 51.4\\%$ chance of the casino winning. This is an example of a Binomial distribution (success/failure); recall that we define a Binomial distribution by:\n",
    "\n",
    "- The probability of success, $p$.\n",
    "- The number of trials undergone, $n$.\n",
    "- The number of successes amongst these trials, $k$.\n",
    "\n",
    "$$\n",
    "P(\\text{win}) = \\frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "Starting with the simplest case, $n=k=1$, $P(\\text{win}) = \\frac{18}{37}$:\n",
    "\n",
    "$$\n",
    "P(\\text{win}) = \\frac{1!}{1!(1-1)!} \\left( \\frac{18}{37} \\right)^1 \\left( 1 - \\frac{18}{37} \\right)^{1-1} = \\frac{18}{37},\n",
    "$$\n",
    "\n",
    "which is as you'd expect, thankfully. As we'll need these metrics going forward, recall the formulae for the mean and standard deviation of a Binomial Distribution:\n",
    "\n",
    "$$\n",
    "\\mu = np,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{np(1-p)}.\n",
    "$$\n",
    "\n",
    "Let us now make an unwise decision and play Roulette 100 times... Our condition for success here is that we win more often than we lose, i.e. we have a target of $k = 51$. How likely are we to achieve this goal?"
   ],
   "id": "b9c55f17c7fa1495"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def binomial(num_trials: int, probability: float) -> list:\n",
    "    trials = range(1, num_trials + 1)\n",
    "    probabilities = []\n",
    "\n",
    "    for trial in trials:\n",
    "        combination = math.comb(num_trials, trial)\n",
    "        trial_probability = probability ** trial * (1 - probability) ** (num_trials - trial)\n",
    "        probabilities.append(combination * trial_probability)\n",
    "\n",
    "    return probabilities"
   ],
   "id": "67a16b45339a460a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trials = 100\n",
    "victory = int((trials / 2) + 1)\n",
    "probabilities = binomial(num_trials=trials, probability=(18 / 37))\n",
    "mean = trials * (18 / 37)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.plot(range(1, trials + 1),\n",
    "         probabilities,\n",
    "         color='k')\n",
    "\n",
    "# Vertical line at \"victory\"\n",
    "plt.vlines(x=victory,\n",
    "           ymin=0,\n",
    "           ymax=max(probabilities) * 1.05,\n",
    "           color='k',\n",
    "           linestyle='--',\n",
    "           linewidth=1.5)\n",
    "\n",
    "# Shaded regions\n",
    "plt.fill_between(range(1, victory),\n",
    "                 probabilities[:victory - 1],\n",
    "                 color='deepskyblue',\n",
    "                 alpha=0.3)\n",
    "\n",
    "plt.fill_between(range(victory, trials + 1),\n",
    "                 probabilities[victory - 1:],\n",
    "                 color='red',\n",
    "                 alpha=0.3)\n",
    "\n",
    "# Arrows (scaled to 20 trials)\n",
    "plt.arrow(victory, max(probabilities) * 0.8,\n",
    "          10, 0,\n",
    "          width=0.001,\n",
    "          length_includes_head=True,\n",
    "          head_width=0.003,\n",
    "          head_length=0.4,\n",
    "          color='k')\n",
    "\n",
    "plt.arrow(victory, max(probabilities) * 0.8,\n",
    "          -10, 0,\n",
    "          width=0.001,\n",
    "          length_includes_head=True,\n",
    "          head_width=0.003,\n",
    "          head_length=0.4,\n",
    "          color='k')\n",
    "\n",
    "# Text labels (scaled)\n",
    "plt.text(victory + 13,\n",
    "         max(probabilities) * 0.80,\n",
    "         \"Win\\nCondition\",\n",
    "         color='r',\n",
    "         ha='left')\n",
    "\n",
    "plt.text(victory - 13,\n",
    "         max(probabilities) * 0.80,\n",
    "         \"Lose\\nCondition\",\n",
    "         color='deepskyblue',\n",
    "         ha='right')\n",
    "\n",
    "plt.xlabel(\"Number of Roulette Wheel Wins\")\n",
    "plt.ylabel(\"Probability of k wins\")\n",
    "plt.xlim(1, trials)\n",
    "plt.ylim(0, max(probabilities) * 1.1)\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "3ac93ad77dc35378"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trials = 1000\n",
    "victory = int((trials / 2) + 1)\n",
    "probabilities = binomial(num_trials=trials, probability=(18 / 37))\n",
    "mean = trials * (18 / 37)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.plot(range(1, trials + 1),\n",
    "         probabilities,\n",
    "         color='k')\n",
    "\n",
    "plt.xlim(350, 650)\n",
    "plt.vlines(x=victory, ymin=0, ymax=0.025, color='k', linestyle='--', linewidth=1.5)\n",
    "\n",
    "# Filling the win/loss regions.\n",
    "plt.fill_between(range(1, victory),\n",
    "                 probabilities[0:victory - 1],\n",
    "                 color='deepskyblue', alpha=0.3)\n",
    "\n",
    "plt.fill_between(range(victory, trials + 1),\n",
    "                 probabilities[victory - 1:trials],\n",
    "                 color='red', alpha=0.3)\n",
    "\n",
    "# Drawing arrows\n",
    "plt.arrow(victory, 0.020, 30, 0,\n",
    "          width=0.0003,\n",
    "          length_includes_head=True,\n",
    "          head_width=0.0006,\n",
    "          head_length=10,\n",
    "          color='k')\n",
    "\n",
    "plt.arrow(victory, 0.020, -30, 0,\n",
    "          width=0.0003,\n",
    "          length_includes_head=True,\n",
    "          head_width=0.0006,\n",
    "          head_length=10,\n",
    "          color='k')\n",
    "\n",
    "# Text in *data* coordinates\n",
    "plt.text(550, 0.020, \"Win\\nCondition\", color='r')\n",
    "\n",
    "# Text in *data* coordinates\n",
    "plt.text(400, 0.020, \"Lose\\nCondition\", color='deepskyblue')\n",
    "\n",
    "plt.xlabel(\"Number of Roulette Wheel Wins\")\n",
    "plt.ylabel(\"Probability of k wins\")\n",
    "plt.show()"
   ],
   "id": "853021ebf1a465c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Conditional Probabilities & Bayes' Theorem\n",
    "\n",
    "In many physical problems, events are not independent. More often than not, the probability of one event can depend on whether another has occurred. The framework in which we describe dependent events is called \"conditional probability\".\n",
    "\n",
    "\n",
    "If we have two events, A and B, the probability of A *given* that B has occurred is written:\n",
    "\n",
    "$$\n",
    "P(A\\, |\\, B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "provided $P(B) \\neq 0$.\n",
    "Here, $P(A \\cap B)$ means the probability that *both* A and B occur.\n",
    "\n",
    "When events are independent, the occurrence of B does not affect A, so\n",
    "\n",
    "$$\n",
    "P(A\\, |\\, B) = P(A)\n",
    "$$\n",
    "\n",
    "and equivalently $P(A \\cap B) = P(A)P(B)$.\n",
    "\n",
    "\n",
    "### Bayes’ theorem\n",
    "Rearranging the definition of conditional probability gives an extremely useful result known as **Bayes’ theorem**:\n",
    "\n",
    "$$\n",
    "P(A\\, |\\, B) = \\frac{P(B\\, |\\, A)\\, P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "This allows us to update our belief about A after observing B.\n",
    "In other words, it connects the **prior** probability $P(A)$ with the **posterior** probability $P(A|B)$.\n",
    "\n",
    "```{admonition} Example: medical testing\n",
    ":class: example\n",
    "\n",
    "A common example used to understand Bayes' theorem is testing for disease. Suppose a disease affects 1% of a population.\n",
    "A test correctly identifies it 99% of the time, but also gives a 5% false-positive rate. In other words, if a person has the disease, the test will return a positive result 99% of the time, and a negative result 1% of the time. Meanwhile, if the person does not have the disease, it will return a negative result 95% of the time, and a positive result 5% of the time.\n",
    "\n",
    "Let A = “person has disease” and B = “test is positive”.\n",
    "Then:\n",
    "\n",
    "- $P(A) = 0.01$\n",
    "- $P(B|A) = 0.99$\n",
    "- $P(B|\\neg A) = 0. 05$\n",
    "\n",
    "The overall probability of obtaining a positive test result must factor in both means of obtaining that result:\n",
    "\n",
    "$$\n",
    "P(B) = P(B|A)P(A) + P(B|\\neg A)P(\\neg A)\n",
    " = 0. 99(0. 01) + 0. 05(0. 99) = 0. 0594\n",
    "$$\n",
    "\n",
    "Applying Bayes’ theorem:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)} = \\frac{0. 99 \\times 0. 01}{0. 0594} \\approx 0. 17\n",
    "$$\n",
    "\n",
    "So even with a positive result, there’s only a 17% chance the person actually has the disease.\n",
    "\n",
    "This illustrates how rare events can strongly influence conditional probabilities. Follow-up testing can help in reducing the uncertainty of a result, by tightening the variance over repeated trials. That is to say, the probability of obtaining several positive test results in a row, given that the person does not have the disease, is astoundingly small, (and is likely indicative that the person does in fact have the disease.)\n",
    "```"
   ],
   "id": "7a07f8d3c8d6c66a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```{admonition} Example: The Monty Hall Problem\n",
    ":class: example\n",
    "\n",
    "The Monty Hall problem is a very famous thought experiment in probability, named after the host, Monty Hall, of the 1970s American television game show *Let’s Make a Deal*. It serves as a canonical example of conditional probability, highlighting how new information can reshape the landscape of a problem even when the underlying setup appears symmetric. The problem is posed as follows: you are on a game show where three doors are presented before you; behind one of these doors is a prize (cash, a car, etc.), whereas behind the other two is a goat. Your objective is to win the prize.\n",
    "\n",
    "You pick a door. Before revealing the outcome of your choice, the host -who knows where the prize is - opens one of the remaining doors to reveal a goat. Crucially, the host will never reveal the prize, or the door that you have picked. You are then offered a choice: stick with your original door, or switch to the remaining unopened one. The puzzle is in determining which is, on average, the more viable strategy. Does switching enhance your odds of winning, or does it make no difference at all? Stop to consider your intuition, and think about what information you are being supplied with as you are making this decision.\n",
    "\n",
    "Let us review the rules of the game:\n",
    "- There are three doors; behind one is a prize, the other two hide goats.\n",
    "- You pick a door randomly.\n",
    "- Before revealing the outcome of your choice, the host opens one of the other doors to reveal a goat.\n",
    "- You are then offered to either remain with your initial choice, or to switch to the remaining unopened door.\n",
    "\n",
    "Let:\n",
    "- $H_i$ denote \"the prize is behind door $i$\", where $i \\in \\{1, 2, 3 \\}$.\n",
    "- $M_i$ denote \"the host opens foor $i$ and reveals a goat\".\n",
    "\n",
    "Before anything happens, the prize is equally likely to be behind any of the doors:\n",
    "\n",
    "$$\n",
    "P(H_1) = P(H_2) = P(H_3) = \\frac{1}{3}\n",
    "$$\n",
    "\n",
    "So if you remain steadfast in your decision, you have a $1/3$ chance of winning. However, the behaviour of the host matters. After they open the door, the problem goes from a generic, random choice to a filtered, curated choice. To see this more clearly, imagine a variant of the original game:\n",
    "- There are 100 doors to pick from in the beginning.\n",
    "- You pick one door.\n",
    "- The host looks at the remaining 99, finds the goats, and opens all but one of them.\n",
    "- You are again left with a choice between two unopened doors.\n",
    "\n",
    "Initially, the door you picked carries a $1/100$ chance of hiding the prize, whereas the remaining 99 doors colelctively carry a $99/100$ chance. The host's action of opening 98 goat doors does not re-randomise the scenario, but effectively concentrates the entire $99/100$ probability mass of \"one of the other doors\" onto the single remaining unopened door. In this exaggerated scenario, it is intuitively clear that you should pick the door that remained after the host's action. The three-door scenario case employs exactly the same logic...\n",
    "\n",
    "Let us now view this problem through the lens of Bayes' Theorem. We have our prior probabilities $P(H_1) = P(H_2) = P(H_3) = \\frac{1}{3}$. Let us say that initially you pick door number 1. Remember that the host has a fixed set of rules:\n",
    "- They never open your chosen door.\n",
    "- They never open the door which hides the prize.\n",
    "- If both of the remaining doors hide goats, they choose the door to open randomly.\n",
    "\n",
    "Now suppose that after choosing door 1, the host opens door 2 to reveal a goat. We now wish to compare:\n",
    "- $P(H_1 | M_2)$ - The probability that the prize is behind the original door.\n",
    "- $P(H_3 | M_2)$ - The probability that the prize is behind the other unopened door.\n",
    "\n",
    "The conditional probabilities on the host's actions are:\n",
    "\n",
    "- If $H_1$ is true, both doors 2 and 3 hide goats, so the host chooses between the two of them randomly.\n",
    "\n",
    "$$\n",
    "P(M_2 | H_1) = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "- If $H_2$ is true, then the host cannot open door 2, so they must open door 3.\n",
    "\n",
    "$$\n",
    "P(M_2 | H_2) = 0\n",
    "$$\n",
    "\n",
    "- If $H_3$ is true, then the host must choose door 2 as the player has originally selected door 1.\n",
    "\n",
    "$$\n",
    "P(M_2 | H_3) = 1\n",
    "$$\n",
    "\n",
    "From this information, we can calculate the probability of the prize being behind one of the two remaining doors with Bayes' Theorem.\n",
    "\n",
    "$$\n",
    "P(H_1 | M_2) = \\frac{P(M_2 | H_1)P(H_1)}{P(M_2)} = \\frac{P(M_2 | H_1)P(H_1)}{P(M_2 | H_1)P(H_1) + P(M_2 | H_2)P(H_2) + P(M_2 | H_3)P(H_3)}.\n",
    "$$\n",
    "\n",
    "We have all the necessary quantities to compute this probability:\n",
    "\n",
    "$$\n",
    "P(H_1 \\mid M_2)\n",
    "= \\frac{\\tfrac{1}{2}\\cdot\\tfrac{1}{3}}\n",
    "       {\\tfrac{1}{2}\\cdot\\tfrac{1}{3} + 0\\cdot\\tfrac{1}{3} + 1\\cdot\\tfrac{1}{3}}\n",
    "= \\frac{\\tfrac{1}{6}}{\\tfrac{1}{2}}\n",
    "= \\frac{1}{3}.\n",
    "$$\n",
    "\n",
    "Similarly,\n",
    "\n",
    "$$\n",
    "P(H_3 | M_2) = \\frac{P(M_2 | H_3)P(H_3)}{P(M_2)} = \\frac{P(M_2 | H_1)P(H_1)}{P(M_2 | H_1)P(H_1) + P(M_2 | H_2)P(H_2) + P(M_2 | H_3)P(H_3)} = \\frac{1 \\cdot \\tfrac{1}{3}}{\\tfrac{1}{2}} = \\frac{2}{3}.\n",
    "$$\n",
    "\n",
    "Therefore, after the host opens door 2 and reveals a goat, your original door still only has probability $\\tfrac{1}{3}$ of hiding the prize, while the other unopened door now carries probability $\\tfrac{2}{3}$. The optimal strategy is to switch doors.\n",
    "```"
   ],
   "id": "9d3b8fefbec276e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6adae0dc32145866"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
